{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gWwj0PX5d-6p"
      },
      "outputs": [],
      "source": [
        "filename = \"requirements.txt\"\n",
        "packs = \"\"\"absl-py==2.3.0\n",
        "certifi\n",
        "charset-normalizer\n",
        "clean-fid\n",
        "colorama\n",
        "contourpy\n",
        "cycler\n",
        "filelock\n",
        "fonttools\n",
        "fsspec\n",
        "grpcio\n",
        "h5py\n",
        "idna\n",
        "imageio\n",
        "Jinja2\n",
        "kiwisolver\n",
        "lazy_loader\n",
        "lightning-utilities\n",
        "Markdown\n",
        "MarkupSafe\n",
        "matplotlib\n",
        "mpmath\n",
        "networkx\n",
        "numpy\n",
        "packaging\n",
        "patchify\n",
        "pillow\n",
        "protobuf\n",
        "pyparsing\n",
        "python-dateutil\n",
        "requests\n",
        "scikit-image\n",
        "scipy\n",
        "six\n",
        "sympy\n",
        "tensorboard\n",
        "tensorboard-data-server\n",
        "tifffile\n",
        "torch\n",
        "torch-fidelity\n",
        "torchaudio\n",
        "torchmetrics\n",
        "torchvision\n",
        "tqdm\n",
        "typing_extensions\n",
        "urllib3\n",
        "Werkzeug\n",
        "piq\n",
        "\"\"\"\n",
        "\n",
        "with open(filename,'w')as f:\n",
        "  f.write(packs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiowmGyyehBA",
        "outputId": "4006d06f-a625-4767-f78f-c784d4ea6649"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: absl-py==2.3.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (2.3.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (2025.6.15)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (3.4.2)\n",
            "Requirement already satisfied: clean-fid in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (0.1.35)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (0.4.6)\n",
            "Requirement already satisfied: contourpy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (1.3.2)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (0.12.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (3.18.0)\n",
            "Requirement already satisfied: fonttools in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (4.58.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 10)) (2025.3.2)\n",
            "Requirement already satisfied: grpcio in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (1.73.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 12)) (3.14.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 13)) (3.10)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 14)) (2.37.0)\n",
            "Requirement already satisfied: Jinja2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 15)) (3.1.6)\n",
            "Requirement already satisfied: kiwisolver in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 16)) (1.4.8)\n",
            "Requirement already satisfied: lazy_loader in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 17)) (0.4)\n",
            "Requirement already satisfied: lightning-utilities in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 18)) (0.14.3)\n",
            "Requirement already satisfied: Markdown in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 19)) (3.8.2)\n",
            "Requirement already satisfied: MarkupSafe in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 20)) (3.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 21)) (3.10.0)\n",
            "Requirement already satisfied: mpmath in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 22)) (1.3.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 23)) (3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 24)) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 25)) (24.2)\n",
            "Requirement already satisfied: patchify in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 26)) (0.2.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 27)) (11.2.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 28)) (5.29.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 29)) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 30)) (2.9.0.post0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 31)) (2.32.3)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 32)) (0.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 33)) (1.15.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 34)) (1.17.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 35)) (1.13.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 36)) (2.18.0)\n",
            "Requirement already satisfied: tensorboard-data-server in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 37)) (0.7.2)\n",
            "Requirement already satisfied: tifffile in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 38)) (2025.6.11)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 39)) (2.6.0+cu124)\n",
            "Requirement already satisfied: torch-fidelity in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 40)) (0.3.0)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 41)) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 42)) (1.7.3)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 43)) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 44)) (4.67.1)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 45)) (4.14.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 46)) (2.4.0)\n",
            "Requirement already satisfied: Werkzeug in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 47)) (3.1.3)\n",
            "Requirement already satisfied: piq in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 48)) (0.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities->-r requirements.txt (line 18)) (75.2.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 39)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 39)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 39)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 39)) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 39)) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 39)) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 39)) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 39)) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 39)) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 39)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 39)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 39)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 39)) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 39)) (3.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZXZEqWpNRuJ2"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import random\n",
        "import tempfile\n",
        "import wandb\n",
        "import yaml\n",
        "import csv\n",
        "import zipfile\n",
        "import uuid\n",
        "import requests\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import sys\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torch.utils.data import Dataset\n",
        "from torch.optim import Adam\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.utils as vutils\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as TF   # For tensor â†” image conversion\n",
        "from torchvision.models import vgg19, VGG19_Weights\n",
        "from torchvision.utils import save_image\n",
        "from torchmetrics.image.fid import FrechetInceptionDistance\n",
        "from tqdm import tqdm\n",
        "from PIL import Image, ImageDraw, ImageFont, ImageFile       # For drawing text and making collages\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
        "from skimage.metrics import peak_signal_noise_ratio as compare_psnr\n",
        "from skimage.metrics import structural_similarity as compare_ssim\n",
        "from piq import ssim  # pip install piq\n",
        "from concurrent.futures import ThreadPoolExecutor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "31fiNZ-CdkA2"
      },
      "outputs": [],
      "source": [
        "#################### Models ########################\n",
        "\n",
        "#==================== SRCNN Model =========================\n",
        "class SRCNN(nn.Module):\n",
        "    \"\"\" Vanilla model of SRCNN with 3 conv layers. \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(SRCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=9, padding=4)\n",
        "        self.conv2 = nn.Conv2d(64, 32, kernel_size=5, padding=2)\n",
        "        self.conv3 = nn.Conv2d(32, 3, kernel_size=5, padding=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.conv3(x)\n",
        "        return x\n",
        "#==========================================================\n",
        "\n",
        "#================== VDSR ========================\n",
        "class VDSR(nn.Module):\n",
        "    def __init__(self, num_channels=1):\n",
        "        super(VDSR, self).__init__()\n",
        "        layers = [nn.Conv2d(num_channels, 64, kernel_size=3, padding=1), nn.ReLU(inplace=True)]\n",
        "        for _ in range(18):\n",
        "            layers.append(nn.Conv2d(64, 64, kernel_size=3, padding=1))\n",
        "            layers.append(nn.ReLU(inplace=True))\n",
        "        layers.append(nn.Conv2d(64, num_channels, kernel_size=3, padding=1))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x) + x  # Residual learning\n",
        "#===========================================================\n",
        "\n",
        "#================ VDSR_Attention Model =====================\n",
        "class VDSR_SA(nn.Module):\n",
        "    def __init__(self, num_channels=3, num_features=64, num_resblocks=18):\n",
        "        super().__init__()\n",
        "        self.input_conv = nn.Conv2d(num_channels, num_features, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.resblocks = nn.Sequential(\n",
        "            *[ResidualBlockSA(num_features) for _ in range(num_resblocks)]\n",
        "        )\n",
        "        #self.output_conv = nn.Conv2d(num_features, num_channels, kernel_size=3, padding=1)\n",
        "        self.output_conv = nn.Sequential(\n",
        "            nn.Conv2d(num_features, num_features // 2, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(num_features // 2, num_channels, 3, padding=1)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.relu(self.input_conv(x))\n",
        "        out = self.resblocks(out)\n",
        "        out = self.output_conv(out)\n",
        "        return out + x  # residual learning\n",
        "\n",
        "class ResidualBlockSA(nn.Module):\n",
        "    def __init__(self, num_features=64):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(num_features, num_features, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(num_features, num_features, kernel_size=3, padding=1)\n",
        "        self.sa = SpatialAttention(kernel_size=7)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.sa(out)\n",
        "        return x + out\n",
        "\n",
        "\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, kernel_size=7):\n",
        "        super().__init__()\n",
        "        assert kernel_size in (3, 7), \"Kernel size must be 3 or 7\"\n",
        "        padding = (kernel_size - 1) // 2\n",
        "\n",
        "        # Compress channels using max-pool and avg-pool and concatenate\n",
        "        self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=padding, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        #self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply max-pool and avg-pool along channel axis (dim=1)\n",
        "        max_pool = torch.max(x, dim=1, keepdim=True)[0]\n",
        "        avg_pool = torch.mean(x, dim=1, keepdim=True)\n",
        "\n",
        "        pool = torch.cat([max_pool, avg_pool], dim=1)  # shape (B, 2, H, W)\n",
        "        attention = self.sigmoid(self.conv(pool))      # shape (B, 1, H, W)\n",
        "        return x * attention\n",
        "#===========================================================\n",
        "\n",
        "#==========================================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EgrYfLUrhHF8"
      },
      "outputs": [],
      "source": [
        "#=================== Data ===================\n",
        "from PIL import ImageFile\n",
        "\n",
        "#================== DIV2K Data Loader ==================\n",
        "class DIV2KDataset(Dataset):\n",
        "    def __init__(self, hr_folder, scale=2, hr_size=(256, 256)):\n",
        "        self.hr_folder = hr_folder\n",
        "        self.hr_files = sorted([f for f in os.listdir(hr_folder) if f.endswith('.png')])\n",
        "        self.scale = scale\n",
        "        self.hr_size = hr_size\n",
        "        self.lr_size = (hr_size[0] // scale, hr_size[1] // scale)\n",
        "\n",
        "        self.hr_transform = transforms.Compose([\n",
        "            transforms.Resize(hr_size, interpolation=Image.BICUBIC),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "\n",
        "        self.lr_transform = transforms.Compose([\n",
        "            transforms.Resize(self.lr_size, interpolation=Image.BICUBIC),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.hr_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        hr_path = os.path.join(self.hr_folder, self.hr_files[idx])\n",
        "        hr_image = Image.open(hr_path).convert('RGB')\n",
        "\n",
        "        hr_tensor = self.hr_transform(hr_image)\n",
        "        lr_tensor = self.lr_transform(hr_image)\n",
        "\n",
        "        return lr_tensor, hr_tensor\n",
        "\n",
        "class TiledDIV2KDataset(Dataset):\n",
        "    def __init__(self, hr_folder=\"Data/DIV2K\", scale=4, crop_size=512, num_threads=8):\n",
        "        self.hr_folder = hr_folder\n",
        "        self.hr_files = sorted([f for f in os.listdir(hr_folder) if f.endswith('.png')])\n",
        "        self.scale = scale\n",
        "        self.crop_size = crop_size\n",
        "        self.lr_size = crop_size // scale\n",
        "        self.transform = transforms.ToTensor()\n",
        "        self.samples = []\n",
        "\n",
        "        print(f\"Preprocessing {len(self.hr_files)} HR images for tiling using {num_threads} threads...\")\n",
        "\n",
        "        def process_image(file_idx_file):\n",
        "            file_idx, file_name = file_idx_file\n",
        "            img_path = os.path.join(self.hr_folder, file_name)\n",
        "            try:\n",
        "                with Image.open(img_path) as img:\n",
        "                    w, h = img.size\n",
        "                tiles_w = w // self.crop_size\n",
        "                tiles_h = h // self.crop_size\n",
        "                return [(file_idx, j * self.crop_size, i * self.crop_size)\n",
        "                        for i in range(tiles_h) for j in range(tiles_w)]\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {file_name}: {e}\")\n",
        "                return []\n",
        "\n",
        "        with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
        "            results = list(tqdm(\n",
        "                executor.map(process_image, enumerate(self.hr_files)),\n",
        "                total=len(self.hr_files),\n",
        "                desc=\"Tiling DIV2K\"\n",
        "            ))\n",
        "\n",
        "        # Flatten list of lists\n",
        "        self.samples = [sample for sublist in results for sample in sublist]\n",
        "\n",
        "        print(f\"Total crop pairs generated: {len(self.samples)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_idx, x, y = self.samples[idx]\n",
        "        file_name = self.hr_files[file_idx]\n",
        "        img_path = os.path.join(self.hr_folder, file_name)\n",
        "\n",
        "        with Image.open(img_path) as img:\n",
        "            hr_image = img.convert(\"RGB\")\n",
        "            hr_crop = hr_image.crop((x, y, x + self.crop_size, y + self.crop_size))\n",
        "            lr_crop = hr_crop.resize((self.lr_size, self.lr_size), Image.BICUBIC)\n",
        "\n",
        "        return self.transform(lr_crop), self.transform(hr_crop)\n",
        "#===========================================================\n",
        "\n",
        "#================== DIV2K Data Utils ==================\n",
        "def download_div2k(destination=\"data\"):\n",
        "    \"\"\"\n",
        "    Downloads the DIV2K dataset zip file, extracts it to 'data/DIV2K',\n",
        "    flattens all subfolders (copies images to a single folder), and removes duplicates.\n",
        "    \"\"\"\n",
        "    os.makedirs(destination, exist_ok=True)\n",
        "    url = \"http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip\"\n",
        "    zip_path = os.path.join(destination, \"DIV2K_train_HR.zip\")\n",
        "    extract_temp = os.path.join(destination, \"_temp_extract\")\n",
        "    final_folder = os.path.join(destination, \"DIV2K\")\n",
        "\n",
        "    # === Download ZIP ===\n",
        "    if not os.path.exists(zip_path):\n",
        "        print(\"Downloading DIV2K...\")\n",
        "        with requests.get(url, stream=True) as r:\n",
        "            r.raise_for_status()\n",
        "            with open(zip_path, 'wb') as f:\n",
        "                for chunk in tqdm(r.iter_content(chunk_size=8192), desc=\"Downloading\"):\n",
        "                    f.write(chunk)\n",
        "    else:\n",
        "        print(\"DIV2K zip already exists.\")\n",
        "\n",
        "    # === Extract ZIP to temp directory ===\n",
        "    if not os.path.exists(final_folder):\n",
        "        print(\"Extracting DIV2K...\")\n",
        "        os.makedirs(extract_temp, exist_ok=True)\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_temp)\n",
        "        print(\"Extraction complete.\")\n",
        "\n",
        "        # Flatten and copy all images to DIV2K directory, remove duplicates\n",
        "        os.makedirs(final_folder, exist_ok=True)\n",
        "        seen = set()\n",
        "        for root, _, files in os.walk(extract_temp):\n",
        "            for file in files:\n",
        "                if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    src_path = os.path.join(root, file)\n",
        "                    dst_path = os.path.join(final_folder, file)\n",
        "                    if file not in seen:\n",
        "                        os.rename(src_path, dst_path)\n",
        "                        seen.add(file)\n",
        "        # Clean up temporary extraction\n",
        "        for root, dirs, files in os.walk(extract_temp, topdown=False):\n",
        "            for name in files:\n",
        "                os.remove(os.path.join(root, name))\n",
        "            for name in dirs:\n",
        "                os.rmdir(os.path.join(root, name))\n",
        "        os.rmdir(extract_temp)\n",
        "        print(\"Files flattened and duplicates removed.\")\n",
        "\n",
        "def preprocess_div2k_center_crop(source_folder=\"Data/DIV2K\", target_folder=\"Data/DIV2K_CROPPED\", patch_size=(512, 512)):\n",
        "    \"\"\"\n",
        "    Extracts a centered patch of size `patch_size` (default 512x512) from each image in 'source_folder'\n",
        "    and saves the patch to 'target_folder'. Skips corrupted or incompatible images.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(target_folder):\n",
        "        os.makedirs(target_folder, exist_ok=True)\n",
        "\n",
        "    print(f\"Cropping center patches of size {patch_size}...\")\n",
        "\n",
        "    for img_name in os.listdir(source_folder):\n",
        "        img_path = os.path.join(source_folder, img_name)\n",
        "        target_path = os.path.join(target_folder, img_name)\n",
        "\n",
        "        try:\n",
        "            with Image.open(img_path) as img:\n",
        "                img.load()  # Ensure full image is read\n",
        "\n",
        "                width, height = img.size\n",
        "                crop_width, crop_height = patch_size\n",
        "\n",
        "                if width < crop_width or height < crop_height:\n",
        "                    print(f\"Skipping {img_name}: image too small ({width}x{height})\")\n",
        "                    continue\n",
        "\n",
        "                left = (width - crop_width) // 2\n",
        "                upper = (height - crop_height) // 2\n",
        "                right = left + crop_width\n",
        "                lower = upper + crop_height\n",
        "\n",
        "                cropped_img = img.crop((left, upper, right, lower))\n",
        "                cropped_img.save(target_path)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping {img_name}: {e}\")\n",
        "\n",
        "    print(\"Center cropping complete. Cropped patches saved to:\", target_folder)\n",
        "\n",
        "\n",
        "def preprocess_div2k(source_folder=\"Data/DIV2K\", target_folder=\"Data/DIV2K_NORMALIZED\", standard_size=(2048, 1408)):\n",
        "    \"\"\"\n",
        "    Resizes all images in 'source_folder' to a fixed standard resolution (default 2048x1408)\n",
        "    and saves them into 'target_folder'. Uses img.load() to catch any corrupted files.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(target_folder):\n",
        "        os.makedirs(target_folder, exist_ok=True)\n",
        "\n",
        "    print(f\"Preprocessing images to standard resolution: {standard_size}...\")\n",
        "\n",
        "    for img_name in os.listdir(source_folder):\n",
        "        img_path = os.path.join(source_folder, img_name)\n",
        "        target_path = os.path.join(target_folder, img_name)\n",
        "\n",
        "        try:\n",
        "            with Image.open(img_path) as img:\n",
        "                img.load()  # ðŸ”¹ Ensure full image is read to trigger exception if corrupted\n",
        "                img = img.resize(standard_size, Image.BICUBIC)\n",
        "                img.save(target_path)\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping {img_name}: {e}\")\n",
        "\n",
        "    print(\"Preprocessing complete. Normalized images saved to:\", target_folder)\n",
        "\n",
        "    def save_batch_as_images(batch_tensor, root_dir):\n",
        "        \"\"\"\n",
        "        Saves a batch of tensors (images) to the specified directory as PNG files.\n",
        "        Each tensor in the batch is saved with a unique UUID filename.\n",
        "        \"\"\"\n",
        "        os.makedirs(root_dir, exist_ok=True)\n",
        "        for i, img in enumerate(batch_tensor):\n",
        "            img = img.clamp(0, 1).cpu()\n",
        "            vutils.save_image(img, os.path.join(root_dir, f\"{uuid.uuid4().hex}.png\"))\n",
        "\n",
        "def resize_lr_images(folder_path, target_size=(512, 512)):\n",
        "    \"\"\"\n",
        "    Iterates over a folder and resizes all image files containing '_lr' in their filename to 512x512.\n",
        "\n",
        "    Args:\n",
        "        folder_path (str): Path to the folder with images.\n",
        "        target_size (tuple): Target size to resize images to. Default is (512, 512).\n",
        "    \"\"\"\n",
        "    valid_exts = (\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tiff\")\n",
        "\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if \"_lr\" in filename and filename.lower().endswith(valid_exts):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "\n",
        "            try:\n",
        "                with Image.open(file_path) as img:\n",
        "                    img_resized = img.resize(target_size, Image.BICUBIC)\n",
        "                    img_resized.save(file_path)\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to process {filename}: {e}\")\n",
        "#===========================================================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dIl8sKh1ksvu"
      },
      "outputs": [],
      "source": [
        "#====================== Metric utils =========================\n",
        "\n",
        "def compute_psnr(sr: torch.Tensor, hr: torch.Tensor, data_range=1.0) -> float:\n",
        "    \"\"\"\n",
        "    Compute average PSNR between super-resolved and high-res images.\n",
        "    Args:\n",
        "        sr: Super-resolved image tensor [B, C, H, W]\n",
        "        hr: High-res ground truth tensor [B, C, H, W]\n",
        "    Returns:\n",
        "        PSNR value averaged over the batch\n",
        "    \"\"\"\n",
        "    sr = sr.clamp(0, 1) # Ensure values are in [0, 1] range\n",
        "    hr = hr.clamp(0, 1) # Ensure values are in [0, 1] range\n",
        "    sr_np = sr.detach().cpu().numpy()\n",
        "    hr_np = hr.detach().cpu().numpy()\n",
        "    psnr = 0.0\n",
        "    for i in range(sr_np.shape[0]):\n",
        "        psnr += peak_signal_noise_ratio(hr_np[i].transpose(1, 2, 0),\n",
        "                                        sr_np[i].transpose(1, 2, 0),\n",
        "                                        data_range=data_range)\n",
        "    return psnr / sr_np.shape[0]\n",
        "\n",
        "def compute_ssim_batch(sr: torch.Tensor, hr: torch.Tensor, data_range=1.0) -> float:\n",
        "    \"\"\"\n",
        "    Compute average SSIM over a batch.\n",
        "    Args:\n",
        "        sr: Super-resolved tensor [B, C, H, W]\n",
        "        hr: High-res ground truth tensor [B, C, H, W]\n",
        "    Returns:\n",
        "        Average SSIM value over batch\n",
        "    \"\"\"\n",
        "    sr = sr.clamp(0, 1) # Ensure values are in [0, 1] range\n",
        "    hr = hr.clamp(0, 1) # Ensure values are in [0, 1] range\n",
        "    sr_np = sr.detach().cpu().numpy()\n",
        "    hr_np = hr.detach().cpu().numpy()\n",
        "    ssim = 0.0\n",
        "    for i in range(sr_np.shape[0]):\n",
        "        ssim += structural_similarity(hr_np[i].transpose(1, 2, 0),\n",
        "                                    sr_np[i].transpose(1, 2, 0),\n",
        "                                    channel_axis=2,\n",
        "                                    data_range=data_range)\n",
        "    return ssim / sr_np.shape[0]\n",
        "\n",
        "#==========================================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0kgz2C7YlMjh"
      },
      "outputs": [],
      "source": [
        "#====================== Plot utils =========================\n",
        "def annotate_image(tensor_img, text):\n",
        "    img = TF.to_pil_image(tensor_img.squeeze(0).cpu())\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    try:\n",
        "        font = ImageFont.truetype(\"arial.ttf\", 16)\n",
        "    except:\n",
        "        font = ImageFont.load_default()\n",
        "    draw.text((5, 5), text, fill=\"white\", font=font)\n",
        "    return TF.to_tensor(img)\n",
        "\n",
        "def create_collage(images, save_path):\n",
        "    widths, heights = zip(*(i.size for i in images))\n",
        "    total_width = sum(widths)\n",
        "    max_height = max(heights)\n",
        "    collage = Image.new('RGB', (total_width, max_height))\n",
        "    x_offset = 0\n",
        "    for img in images:\n",
        "        collage.paste(img, (x_offset, 0))\n",
        "        x_offset += img.size[0]\n",
        "    collage.save(save_path)\n",
        "\n",
        "\n",
        "# === Training Curves Plot ===\n",
        "def plot_training_curves(history, model_name, save_path=None):\n",
        "\n",
        "    epochs = range(1, len(history['train_loss']) + 1)\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(epochs, history['train_loss'], label='Train Loss')\n",
        "    if 'val_loss' in history:\n",
        "        plt.plot(epochs, history['val_loss'], label='Val Loss')\n",
        "    plt.plot(epochs, history['val_psnr'], label='Val PSNR')\n",
        "    plt.plot(epochs, history['val_ssim'], label='Val SSIM')\n",
        "    if any(history.get('val_fid', [])):\n",
        "        plt.plot(epochs, [fid if fid is not None else np.nan for fid in history['val_fid']], label='Val FID')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Metric')\n",
        "    plt.legend()\n",
        "    plt.title(f\"Training Curves - {model_name}\")\n",
        "    if save_path:\n",
        "        plt.savefig(save_path)\n",
        "    plt.show()\n",
        "\n",
        "#==========================================================="
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#====================== Results Logger =========================\n",
        "def log_result(\n",
        "    model_name,\n",
        "    loss_type,\n",
        "    metrics,\n",
        "    save_dir,\n",
        "    final_train_loss=None,\n",
        "    final_val_loss=None,\n",
        "    csv_path=\"results.csv\"\n",
        "):\n",
        "    \"\"\"Append model results to a central CSV log with timestamps and losses.\"\"\"\n",
        "    fieldnames = [\n",
        "        \"timestamp_unix\",\n",
        "        \"timestamp_readable\",\n",
        "        \"model\",\n",
        "        \"loss\",\n",
        "        \"train_loss\",\n",
        "        \"val_loss\",\n",
        "        \"psnr\",\n",
        "        \"ssim\",\n",
        "        \"fid\",\n",
        "        \"save_dir\"\n",
        "    ]\n",
        "    file_exists = os.path.exists(csv_path)\n",
        "\n",
        "    timestamp_unix = int(time.time())\n",
        "    timestamp_readable = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "    row = {\n",
        "        \"timestamp_unix\": timestamp_unix,\n",
        "        \"timestamp_readable\": timestamp_readable,\n",
        "        \"model\": model_name,\n",
        "        \"loss\": loss_type,\n",
        "        \"train_loss\": final_train_loss,\n",
        "        \"val_loss\": final_val_loss,\n",
        "        \"psnr\": metrics.get(\"test_psnr\"),\n",
        "        \"ssim\": metrics.get(\"test_ssim\"),\n",
        "        \"fid\": metrics.get(\"test_fid\"),\n",
        "        \"save_dir\": save_dir\n",
        "    }\n",
        "\n",
        "    with open(csv_path, \"a\", newline=\"\") as csvfile:\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "        if not file_exists:\n",
        "            writer.writeheader()\n",
        "        writer.writerow(row)\n",
        "#==============================================================="
      ],
      "metadata": {
        "id": "C4KiKRQM04kJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ou2fVeLkmkQM"
      },
      "outputs": [],
      "source": [
        "#====================== Loss Functions =========================\n",
        "class PerceptualLoss(nn.Module):\n",
        "    def __init__(self, device=\"cpu\"):\n",
        "        super().__init__()\n",
        "\n",
        "        # Load pretrained VGG19 and use layers up to relu2_2 (layer 8)\n",
        "        vgg = vgg19(weights=VGG19_Weights.DEFAULT).features[:9].to(device).eval()\n",
        "\n",
        "        # Freeze VGG parameters\n",
        "        for param in vgg.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.feature_extractor = vgg\n",
        "        self.criterion = nn.MSELoss()\n",
        "\n",
        "        # Normalize to match VGG19 input expectations\n",
        "        self.normalize = transforms.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225]\n",
        "        )\n",
        "\n",
        "    def forward(self, sr, hr):\n",
        "        \"\"\"\n",
        "        sr, hr: [B, 3, H, W] images in [0, 1] range\n",
        "        \"\"\"\n",
        "        # Normalize each image in batch\n",
        "        sr_norm = torch.stack([self.normalize(img) for img in sr])\n",
        "        hr_norm = torch.stack([self.normalize(img) for img in hr])\n",
        "\n",
        "        sr_feat = self.feature_extractor(sr_norm)\n",
        "        hr_feat = self.feature_extractor(hr_norm)\n",
        "\n",
        "        return self.criterion(sr_feat, hr_feat)\n",
        "\n",
        "class NewCombinedLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.2, beta=0.4, data_range=1.0):\n",
        "        \"\"\"\n",
        "        alpha: weight for MSE\n",
        "        beta: weight for L1\n",
        "        (1 - alpha - beta): weight for SSIM\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.data_range = data_range\n",
        "        self.mse = nn.MSELoss()\n",
        "        self.l1 = nn.L1Loss()\n",
        "\n",
        "    def forward(self, prediction, target):\n",
        "        mse_loss = self.mse(prediction, target)\n",
        "        l1_loss = self.l1(prediction, target)\n",
        "        prediction = prediction.clamp(0,1)\n",
        "        target = target.clamp(0,1)\n",
        "        ssim_val = ssim(prediction, target, data_range=self.data_range)\n",
        "        ssim_loss = 1 - ssim_val  # Higher SSIM = better, so loss = 1 - SSIM\n",
        "        return self.alpha * mse_loss + self.beta * l1_loss + (1 - self.alpha - self.beta) * ssim_loss\n",
        "\n",
        "class CombinedLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.8, resize=True, device='cpu'):\n",
        "        super().__init__()\n",
        "        self.perceptual = PerceptualLoss(device=device)\n",
        "        self.alpha = alpha\n",
        "        self.mse = nn.MSELoss()\n",
        "\n",
        "    def forward(self, sr, hr):\n",
        "        l2 = self.mse(sr, hr)\n",
        "        perceptual = self.perceptual(sr, hr)\n",
        "        return self.alpha * l2 + (1 - self.alpha) * perceptual\n",
        "\n",
        "class CharbonnierLoss(torch.nn.Module):\n",
        "    def __init__(self, eps=1e-3):\n",
        "        super(CharbonnierLoss, self).__init__()\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        return torch.mean(torch.sqrt((pred - target) ** 2 + self.eps ** 2))\n",
        "#==========================================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_1K_TgtknFb7"
      },
      "outputs": [],
      "source": [
        "#====================== Training =========================\n",
        "\n",
        "def train_and_validate(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    optimizer,\n",
        "    loss_fn,\n",
        "    save_dir,\n",
        "    checkpoint_dir=\"checkpoints\",\n",
        "    model_name=\"Model\",\n",
        "    num_epochs=20,\n",
        "    val_fid_interval=5,\n",
        "    device=None,\n",
        "    verbose=True,\n",
        "    early_stopping_patience=10,\n",
        "    use_wandb=False\n",
        "):\n",
        "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    best_val_psnr = 0\n",
        "    early_stop_counter = 0\n",
        "    history = {'train_loss': [], 'val_loss': [], 'val_psnr': [], 'val_ssim': [], 'val_fid': []}\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for lr_img, hr_img in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "            lr_img, hr_img = lr_img.to(device), hr_img.to(device)\n",
        "            lr_up = F.interpolate(lr_img, size=hr_img.shape[-2:], mode='bicubic', align_corners=False)\n",
        "            output = model(lr_up).clamp(0, 1)\n",
        "            loss = loss_fn(output, hr_img)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = running_loss / len(train_loader)\n",
        "        scheduler.step(avg_train_loss)\n",
        "        history['train_loss'].append(avg_train_loss)\n",
        "\n",
        "        # === Validation ===\n",
        "        model.eval()\n",
        "        val_loss_total = 0.0\n",
        "        psnr_list, ssim_list = [], []\n",
        "        with torch.no_grad():\n",
        "            for lr_img, hr_img in val_loader:\n",
        "                lr_img, hr_img = lr_img.to(device), hr_img.to(device)\n",
        "                lr_up = F.interpolate(lr_img, size=hr_img.shape[-2:], mode='bicubic', align_corners=False)\n",
        "                output = model(lr_up).clamp(0, 1)\n",
        "\n",
        "                val_loss_total += loss_fn(output, hr_img).item()\n",
        "                psnr_list.append(compute_psnr(output, hr_img))\n",
        "                ssim_list.append(compute_ssim_batch(output, hr_img))\n",
        "\n",
        "        avg_val_loss = val_loss_total / len(val_loader)\n",
        "        val_psnr = np.mean(psnr_list)\n",
        "        val_ssim = np.mean(ssim_list)\n",
        "\n",
        "        history['val_loss'].append(avg_val_loss)\n",
        "        history['val_psnr'].append(val_psnr)\n",
        "        history['val_ssim'].append(val_ssim)\n",
        "\n",
        "        # === Optional FID ===\n",
        "        if (epoch + 1) % val_fid_interval == 0:\n",
        "            fid_metric = FrechetInceptionDistance(feature=2048, normalize=True).to(device)\n",
        "            with torch.no_grad():\n",
        "                for lr_img, hr_img in val_loader:\n",
        "                    lr_img, hr_img = lr_img.to(device), hr_img.to(device)\n",
        "                    lr_up = F.interpolate(lr_img, size=hr_img.shape[-2:], mode='bicubic', align_corners=False)\n",
        "                    output = model(lr_up).clamp(0, 1)\n",
        "                    sr_resized = F.interpolate(output, size=(299, 299), mode='bilinear', align_corners=False)\n",
        "                    hr_resized = F.interpolate(hr_img, size=(299, 299), mode='bilinear', align_corners=False)\n",
        "                    fid_metric.update(sr_resized, real=False)\n",
        "                    fid_metric.update(hr_resized, real=True)\n",
        "                fid_score = fid_metric.compute().item()\n",
        "                history['val_fid'].append(fid_score)\n",
        "        else:\n",
        "            fid_score = None\n",
        "            history['val_fid'].append(None)\n",
        "\n",
        "        # === Logging ===\n",
        "        if verbose:\n",
        "            print(f\"Epoch {epoch+1}: Train Loss={avg_train_loss:.4f}, Val Loss={avg_val_loss:.4f}, \"\n",
        "                  f\"PSNR={val_psnr:.2f}, SSIM={val_ssim:.4f}, FID={fid_score}\")\n",
        "\n",
        "        if use_wandb:\n",
        "            import wandb\n",
        "            wandb.log({\n",
        "                \"epoch\": epoch + 1,\n",
        "                \"train_loss\": avg_train_loss,\n",
        "                \"val_loss\": avg_val_loss,\n",
        "                \"val_psnr\": val_psnr,\n",
        "                \"val_ssim\": val_ssim,\n",
        "                \"val_fid\": fid_score\n",
        "            })\n",
        "\n",
        "        # === Checkpointing ===\n",
        "        if val_psnr > best_val_psnr:\n",
        "            best_val_psnr = val_psnr\n",
        "            early_stop_counter = 0\n",
        "            torch.save(model.state_dict(), os.path.join(checkpoint_dir, model_name + '_best_model.pth'))\n",
        "        else:\n",
        "            early_stop_counter += 1\n",
        "\n",
        "        if early_stop_counter >= early_stopping_patience:\n",
        "            if verbose:\n",
        "                print(f\"\\nEarly stopping at epoch {epoch+1}. No improvement in PSNR for {early_stopping_patience} epochs.\")\n",
        "            break\n",
        "\n",
        "    # === Append training history to metrics.json ===\n",
        "    metrics_path = os.path.join(save_dir, \"metrics.json\")\n",
        "\n",
        "    with open(metrics_path, \"w\") as f:\n",
        "        #json.dump(existing_data, f, indent=2)\n",
        "        json.dump(history, f, indent=2)\n",
        "\n",
        "    return model, history\n",
        "\n",
        "\n",
        "def test_upsample(\n",
        "    model,\n",
        "    test_loader,\n",
        "    save_dir,\n",
        "    checkpoint_dir=\"checkpoints\",\n",
        "    model_name=\"Model\",\n",
        "    forced_indices=None,\n",
        "    device=None,\n",
        "    verbose=True,\n",
        "    use_wandb=False\n",
        "):\n",
        "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    best_model_path = os.path.join(checkpoint_dir, model_name + '_best_model.pth')\n",
        "    if os.path.exists(best_model_path):\n",
        "        model.load_state_dict(torch.load(best_model_path))\n",
        "        if verbose:\n",
        "            print(f\"Loaded best model from checkpoint: {best_model_path}\")\n",
        "\n",
        "    model.eval()\n",
        "    psnr_list, ssim_list = [], []\n",
        "    collage_dir = Path(save_dir) / \"collages\"\n",
        "    example_dir = Path(save_dir) / \"test_examples\"\n",
        "    os.makedirs(collage_dir, exist_ok=True)\n",
        "    os.makedirs(example_dir, exist_ok=True)\n",
        "\n",
        "    example_data = {}\n",
        "    dataset = test_loader.dataset\n",
        "\n",
        "    if forced_indices is None:\n",
        "        forced_indices = sorted(random.sample(range(len(dataset)), 10))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx in forced_indices:\n",
        "            lr_img, hr_img = dataset[idx]\n",
        "            lr_img, hr_img = lr_img.unsqueeze(0).to(device), hr_img.unsqueeze(0).to(device)\n",
        "            lr_up = F.interpolate(lr_img, size=hr_img.shape[-2:], mode='bicubic', align_corners=False)\n",
        "            output = model(lr_up).clamp(0, 1)\n",
        "\n",
        "            psnr = compute_psnr(output, hr_img)\n",
        "            ssim = compute_ssim_batch(output, hr_img)\n",
        "            psnr_list.append(psnr)\n",
        "            ssim_list.append(ssim)\n",
        "\n",
        "            collage = [TF.to_pil_image(t.squeeze(0).cpu()) for t in [lr_img, output, hr_img]]\n",
        "            collage_path = collage_dir / f\"{idx:05d}_PSNR{psnr:.2f}_SSIM{ssim:.4f}.png\"\n",
        "            resize_lr_images(example_dir, target_size=(512, 512))\n",
        "            create_collage(collage, collage_path)\n",
        "\n",
        "            paths = {\n",
        "                \"lr\": example_dir / f\"{idx}_lr.png\",\n",
        "                \"sr\": example_dir / f\"{idx}_sr.png\",\n",
        "                \"hr\": example_dir / f\"{idx}_hr.png\"\n",
        "            }\n",
        "\n",
        "            save_image(lr_img.clamp(0, 1), paths[\"lr\"])\n",
        "            save_image(output.clamp(0, 1), paths[\"sr\"])\n",
        "            save_image(hr_img.clamp(0, 1), paths[\"hr\"])\n",
        "\n",
        "            example_data[idx] = {\n",
        "                \"lr\": str(paths[\"lr\"]),\n",
        "                \"sr\": str(paths[\"sr\"]),\n",
        "                \"hr\": str(paths[\"hr\"]),\n",
        "                \"psnr\": float(psnr),\n",
        "                \"ssim\": float(ssim)\n",
        "            }\n",
        "\n",
        "    fid_metric = FrechetInceptionDistance(feature=2048, normalize=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        for entry in example_data.values():\n",
        "            sr = TF.to_tensor(Image.open(entry[\"sr\"]).convert(\"RGB\")).unsqueeze(0).to(device)\n",
        "            hr = TF.to_tensor(Image.open(entry[\"hr\"]).convert(\"RGB\")).unsqueeze(0).to(device)\n",
        "            sr = F.interpolate(sr, size=(299, 299), mode='bilinear', align_corners=False)\n",
        "            hr = F.interpolate(hr, size=(299, 299), mode='bilinear', align_corners=False)\n",
        "            fid_metric.update(sr, real=False)\n",
        "            fid_metric.update(hr, real=True)\n",
        "\n",
        "    final_metrics = {\n",
        "        \"test_psnr\": float(np.mean(psnr_list)),\n",
        "        \"test_ssim\": float(np.mean(ssim_list)),\n",
        "        \"test_fid\": float(fid_metric.compute().item())\n",
        "    }\n",
        "\n",
        "    if verbose:\n",
        "        print(\"\\n=== Final Test Metrics ===\")\n",
        "        for k, v in final_metrics.items():\n",
        "            print(f\"{k.upper()}: {v:.4f}\")\n",
        "\n",
        "    metrics_path = os.path.join(save_dir, 'metrics.json')\n",
        "\n",
        "    # Load existing history from metrics.json if it exists\n",
        "    if os.path.exists(metrics_path):\n",
        "        try:\n",
        "            with open(metrics_path, 'r') as f:\n",
        "                metrics_data = json.load(f)\n",
        "            if not isinstance(metrics_data, dict):\n",
        "                print(\"Warning: metrics.json is not a valid dictionary. Resetting.\")\n",
        "                metrics_data = {}\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading metrics.json: {e}. Resetting.\")\n",
        "            metrics_data = {}\n",
        "    else:\n",
        "        metrics_data = {}\n",
        "\n",
        "    # Update the loaded dictionary with test results\n",
        "    metrics_data.update(final_metrics)\n",
        "\n",
        "    # Save updated dictionary (overwriting the file)\n",
        "    with open(metrics_path, 'w') as f:\n",
        "        json.dump(metrics_data, f, indent=2)\n",
        "\n",
        "    # Save test examples\n",
        "    with open(Path(save_dir) / \"test_examples.json\", 'w') as f:\n",
        "        json.dump(example_data, f, indent=2)\n",
        "\n",
        "    if use_wandb:\n",
        "        wandb.log(final_metrics)\n",
        "        for idx in list(example_data.keys())[:3]:  # log 3 example images\n",
        "            wandb.log({\n",
        "                f\"Example_{idx}\": [\n",
        "                    wandb.Image(str(example_data[idx][\"lr\"]), caption=\"LR\"),\n",
        "                    wandb.Image(str(example_data[idx][\"sr\"]), caption=\"SR\"),\n",
        "                    wandb.Image(str(example_data[idx][\"hr\"]), caption=\"HR\"),\n",
        "                ]\n",
        "            })\n",
        "\n",
        "    return final_metrics, example_data\n",
        "#==========================================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4KfxmanGjGLr"
      },
      "outputs": [],
      "source": [
        "# ---------------------- Channel Attention Layer ----------------------\n",
        "class CALayer(nn.Module):\n",
        "    def __init__(self, channel, reduction=8):\n",
        "        super(CALayer, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Conv2d(channel, channel // reduction, 1, bias=True),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(channel // reduction, channel, 1, bias=True),\n",
        "            #nn.Sigmoid()\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.avg_pool(x)\n",
        "        y = self.fc(y)\n",
        "        return x * y\n",
        "\n",
        "# ---------------------- Pyramid Deep SRCNN with Channel Attention ----------------------\n",
        "class PyramidDeepSRCNN_CA(nn.Module):\n",
        "    def __init__(self, num_channels=3):\n",
        "        super(PyramidDeepSRCNN_CA, self).__init__()\n",
        "\n",
        "        # Entry layer\n",
        "        self.entry = nn.Sequential(\n",
        "            nn.Conv2d(num_channels, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # Progressive channel expansion\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # Deep middle blocks (64 channels)\n",
        "        self.middle_blocks = nn.Sequential(\n",
        "            *[\n",
        "                nn.Sequential(\n",
        "                    nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "                    nn.ReLU(inplace=True)\n",
        "                ) for _ in range(6)\n",
        "            ],\n",
        "            CALayer(64)\n",
        "        )\n",
        "\n",
        "        # Progressive channel reduction\n",
        "        self.deconv1 = nn.Sequential(\n",
        "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.deconv2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # Output layer\n",
        "        self.exit = nn.Conv2d(16, num_channels, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.entry(x)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.middle_blocks(x)\n",
        "        x = self.deconv1(x)\n",
        "        x = self.deconv2(x)\n",
        "        x = self.exit(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_pipeline(config):\n",
        "    print(\"Downloading and preparing DIV2K dataset...\")\n",
        "    download_div2k(\"Data\")\n",
        "\n",
        "    # Set all random seeds for reproducibility\n",
        "    seed = 42\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    generator = torch.Generator().manual_seed(seed)\n",
        "\n",
        "    # Apply augmentation if requested\n",
        "    if config[\"aug\"]:\n",
        "        add_augmentation(\"Data/DIV2K\")\n",
        "\n",
        "    # Load dataset\n",
        "    if config[\"tiled\"]:\n",
        "        print(\"Using TiledDIV2KDataset\")\n",
        "        dataset = TiledDIV2KDataset(\"Data/DIV2K\", scale=config[\"scale\"])\n",
        "    else:\n",
        "        print(\"Using DIV2KDataset\")\n",
        "        dataset = DIV2KDataset(\"Data/DIV2K\", scale=config[\"scale\"])\n",
        "\n",
        "    # Ensure deterministic splitting\n",
        "    total_size = len(dataset)\n",
        "    train_size = int(0.8 * total_size)\n",
        "    val_size = int(0.1 * total_size)\n",
        "    test_size = total_size - train_size - val_size\n",
        "\n",
        "    train_set, val_set, test_set = random_split(\n",
        "        dataset, [train_size, val_size, test_size], generator=generator\n",
        "    )\n",
        "\n",
        "    # Set shuffle=False to maintain consistent index-to-image mapping\n",
        "    train_loader = DataLoader(train_set, batch_size=config[\"batch_size\"], shuffle=False)\n",
        "    val_loader = DataLoader(val_set, batch_size=1, shuffle=False)\n",
        "    test_loader = DataLoader(test_set, batch_size=1, shuffle=False)\n",
        "\n",
        "    forced_indices = config[\"idx\"]\n",
        "\n",
        "    print(\"Dataset ready and reproducible.\")\n",
        "    return train_loader, val_loader, test_loader, forced_indices\n",
        "\n",
        "def test_pipeline(config, test_loader, forced_indices, device, history=None):\n",
        "    model_name = config[\"model\"]\n",
        "\n",
        "    # === Model Instantiation ===\n",
        "    if model_name == \"SRCNN\":\n",
        "        model = SRCNN()\n",
        "        test_fn = test_model_with_upsample\n",
        "    elif model_name == \"SvOcSRCNN\":\n",
        "        model = SvOcSRCNN()\n",
        "        test_fn = test_model_with_upsample\n",
        "    elif model_name == \"VDSR\":\n",
        "        model = VDSR(num_channels=3)\n",
        "        test_fn = test_model_with_upsample\n",
        "    elif model_name == \"VDSR_SA\":\n",
        "        model = VDSR_SA(num_features=64, num_resblocks=24)\n",
        "        test_fn = test_model_with_upsample\n",
        "    elif model_name == \"dsrcnn_ca\":\n",
        "        model = PyramidDeepSRCNN_CA(num_channels=3)\n",
        "        test_fn = test_model_with_upsample\n",
        "    elif model_name == \"RCAN\":\n",
        "        model = RCAN(num_channels=3, scale=config[\"scale\"])\n",
        "        test_fn = test_model_no_upsample\n",
        "    elif model_name == \"RCAN_SWIN\":\n",
        "        model = RCAN_Swin(num_channels=3)\n",
        "        test_fn = test_model_no_upsample\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported model in test_pipeline: {model_name}\")\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    # === Run the test function ===\n",
        "    metrics, example_data = test_fn(\n",
        "        model=model,\n",
        "        test_loader=test_loader,\n",
        "        save_dir=config[\"save_dir\"],\n",
        "        checkpoint_dir=\"checkpoints\",\n",
        "        model_name=model_name,\n",
        "        forced_indices=forced_indices,\n",
        "        device=device,\n",
        "        use_wandb=config.get(\"use_wandb\", False),\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    if config.get(\"use_wandb\", False):\n",
        "        wandb.log(metrics)\n",
        "\n",
        "    log_result(model_name, config[\"loss\"], metrics, config[\"save_dir\"])\n",
        "    generate_summary_collage_from_checkpoints()\n",
        "\n",
        "    print(f\"Testing complete for model: {model_name}\")\n",
        "\n",
        "    final_train_loss = history['train_loss'][-1] if history and 'train_loss' in history else None\n",
        "    final_val_loss = history['val_loss'][-1] if history and 'val_loss' in history else None\n",
        "\n",
        "    log_result(\n",
        "    model_name=config[\"model\"],\n",
        "    loss_type=config[\"loss\"],\n",
        "    metrics=metrics,\n",
        "    save_dir=config[\"save_dir\"],\n",
        "    final_train_loss=final_train_loss,\n",
        "    final_val_loss=final_val_loss\n",
        "    )\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def get_loss_fn(name,device=None):\n",
        "    if name == \"mse\":\n",
        "        return nn.MSELoss()\n",
        "    elif name == \"charbonnier\":\n",
        "        return CharbonnierLoss()\n",
        "    elif name == \"combined\":\n",
        "        return CombinedLoss(alpha=0.8, device=device)\n",
        "    elif name == \"NewCombinedLoss\":\n",
        "        return NewCombinedLoss(alpha=0.2, beta=0.6)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported loss function: \" + name)\n",
        "\n",
        "def train_pipeline(config, train_loader, val_loader, device):\n",
        "\n",
        "    model_name = config[\"model\"]\n",
        "    loss_fn = get_loss_fn(config[\"loss\"], device)\n",
        "    lr = float(config.get(\"lr\", 1e-4))\n",
        "\n",
        "    # === Model Selection ===\n",
        "    if model_name == \"SRCNN\":\n",
        "        model = SRCNN().to(device)\n",
        "    elif model_name == \"SvOcSRCNN\":\n",
        "        model = SvOcSRCNN().to(device)\n",
        "    elif model_name == \"VDSR_SA\":\n",
        "        model = VDSR_SA(num_features=64, num_resblocks=15).to(device)\n",
        "    elif model_name == \"VDSR\":\n",
        "        model = VDSR(num_channels=3).to(device)\n",
        "    elif model_name == \"dsrcnn_ca\":\n",
        "        model = PyramidDeepSRCNN_CA(num_channels=3).to(device)\n",
        "    elif model_name == \"RCAN_SWIN\":\n",
        "        model = RCAN_Swin(num_channels=3).to(device)\n",
        "    elif model_name == \"RCAN\":\n",
        "        model = RCAN(num_channels=3, scale=config[\"scale\"]).to(device)\n",
        "    else:\n",
        "        raise ValueError(f\"Model {model_name} not supported in train_pipeline.\")\n",
        "\n",
        "    optimizer = Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    print(f\"Training model: {model_name} with loss: {config['loss']}\")\n",
        "\n",
        "    if model_name in [\"RCAN\", \"RCAN_SWIN\"]:\n",
        "        trained_model, history, _ = train_no_upsample(\n",
        "            model=model,\n",
        "            train_loader=train_loader,\n",
        "            val_loader=val_loader,\n",
        "            test_loader=None,  # not used in training phase\n",
        "            optimizer=optimizer,\n",
        "            loss_fn=loss_fn,\n",
        "            model_name=model_name,\n",
        "            save_dir=config[\"save_dir\"],\n",
        "            checkpoint_dir=\"checkpoints\",\n",
        "            num_epochs=config[\"epochs\"],\n",
        "            device=device,\n",
        "            forced_indices=None,\n",
        "            verbose=True,\n",
        "            early_stopping_patience=10\n",
        "        )\n",
        "    else:\n",
        "        trained_model, history = train_and_validate(\n",
        "            model=model,\n",
        "            train_loader=train_loader,\n",
        "            val_loader=val_loader,\n",
        "            optimizer=optimizer,\n",
        "            loss_fn=loss_fn,\n",
        "            save_dir=config[\"save_dir\"],\n",
        "            checkpoint_dir=\"checkpoints\",\n",
        "            model_name=model_name,\n",
        "            num_epochs=config[\"epochs\"],\n",
        "            device=device,\n",
        "            use_wandb=config.get(\"use_wandb\", False),\n",
        "            early_stopping_patience=10\n",
        "        )\n",
        "\n",
        "    print(f\"Training completed for model: {model_name}\")\n",
        "    return trained_model, history"
      ],
      "metadata": {
        "id": "rUZCcdSV5aA8"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "hVtTlxvpUOOg"
      },
      "outputs": [],
      "source": [
        "def create_multi_model_collage(root_dir: str = \"checkpoints\", font_path: str = None, font_size: int = 22):\n",
        "    \"\"\"\n",
        "    Generates a comparison collage of model outputs from multiple models.\n",
        "    Each row represents an example index, and each column represents a model's SR image with LR and HR as the first two columns.\n",
        "    \"\"\"\n",
        "    root = Path(root_dir)\n",
        "    model_dirs = [d for d in root.iterdir() if d.is_dir() and (d / \"test_examples.json\").exists() and (d / \"metrics.json\").exists()]\n",
        "    model_dirs = sorted(model_dirs, key=lambda d: d.name)\n",
        "\n",
        "    if not model_dirs:\n",
        "        raise ValueError(\"No valid model directories found.\")\n",
        "\n",
        "    # Load test examples and ensure consistent example indices\n",
        "    model_data = {}\n",
        "    example_indices = None\n",
        "    for model_dir in model_dirs:\n",
        "        name = model_dir.name\n",
        "        with open(model_dir / \"test_examples.json\") as f:\n",
        "            examples = json.load(f)\n",
        "        with open(model_dir / \"metrics.json\") as f:\n",
        "            metrics = json.load(f)\n",
        "\n",
        "        indices = sorted(map(int, examples.keys()))\n",
        "        if example_indices is None:\n",
        "            example_indices = indices\n",
        "        elif example_indices != indices:\n",
        "            raise ValueError(f\"Example indices do not match across models. Check model {name}.\")\n",
        "\n",
        "        model_data[name] = {\"examples\": examples, \"metrics\": metrics}\n",
        "\n",
        "    num_examples = len(example_indices)\n",
        "    num_models = len(model_data)\n",
        "    columns = [\"LR\", *model_data.keys(), \"HR\"]\n",
        "    cell_width, cell_height = 400, 400 + font_size + 10\n",
        "    font = ImageFont.truetype(font_path or str(ImageFont.load_default().path), font_size) if font_path else ImageFont.load_default()\n",
        "\n",
        "    for example_idx in example_indices:\n",
        "        collage = Image.new(\"RGB\", (len(columns) * cell_width, cell_height), (255, 255, 255))\n",
        "        draw = ImageDraw.Draw(collage)\n",
        "\n",
        "        for col_idx, label in enumerate(columns):\n",
        "            x = col_idx * cell_width\n",
        "            if label == \"LR\":\n",
        "                img_path = model_dirs[0] / \"test_examples\" / f\"{example_idx}_lr.png\"\n",
        "                caption = \"Low Resolution\"\n",
        "            elif label == \"HR\":\n",
        "                img_path = model_dirs[0] / \"test_examples\" / f\"{example_idx}_hr.png\"\n",
        "                caption = \"High Resolution\"\n",
        "            else:\n",
        "                img_path = Path(model_data[label][\"examples\"][str(example_idx)][\"sr\"])\n",
        "                psnr = model_data[label][\"examples\"][str(example_idx)][\"psnr\"]\n",
        "                ssim = model_data[label][\"examples\"][str(example_idx)][\"ssim\"]\n",
        "                fid = model_data[label][\"metrics\"][\"test_fid\"]\n",
        "                caption = f\"{label}\\nPSNR: {psnr:.2f}, SSIM: {ssim:.4f}, FID: {fid:.2f}\"\n",
        "\n",
        "            if not img_path.exists():\n",
        "                continue\n",
        "\n",
        "            img = Image.open(img_path).convert(\"RGB\").resize((cell_width, cell_width), Image.BICUBIC)\n",
        "            collage.paste(img, (x, 0))\n",
        "            draw.rectangle([(x, cell_width), (x + cell_width, cell_height)], fill=(255, 255, 255))\n",
        "            draw.text((x + 5, cell_width + 2), caption, fill=(0, 0, 0), font=font)\n",
        "\n",
        "        out_path = root /\"summary_collages\"/ f\"{example_idx:03d}_comparison_collage.png\"\n",
        "        collage.save(out_path)\n",
        "        print(f\"Saved collage for example {example_idx} to {out_path}\")\n",
        "\n",
        "def generate_summary_collage_from_checkpoints(checkpoints_root=\"checkpoints\", output_dir=\"checkpoints/summary_collages\"):\n",
        "    model_dirs = [d for d in os.listdir(checkpoints_root) if os.path.isdir(os.path.join(checkpoints_root, d)) and d != \"summary_collages\"]\n",
        "\n",
        "    all_model_outputs = {}\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "    for model_name in model_dirs:\n",
        "        json_path = os.path.join(checkpoints_root, model_name, \"test_examples.json\")\n",
        "        if not os.path.exists(json_path):\n",
        "            print(f\"Skipping {model_name}, no test_examples.json found.\")\n",
        "            continue\n",
        "\n",
        "        with open(json_path, \"r\") as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        all_model_outputs[model_name] = {}\n",
        "        for idx_str, entry in data.items():\n",
        "            idx = int(idx_str)\n",
        "            all_model_outputs[model_name][idx] = {\n",
        "                \"lr\": TF.to_tensor(Image.open(entry[\"lr\"]).convert(\"RGB\")).unsqueeze(0),\n",
        "                \"sr\": TF.to_tensor(Image.open(entry[\"sr\"]).convert(\"RGB\")).unsqueeze(0),\n",
        "                \"hr\": TF.to_tensor(Image.open(entry[\"hr\"]).convert(\"RGB\")).unsqueeze(0),\n",
        "                \"psnr\": entry[\"psnr\"],\n",
        "                \"ssim\": entry[\"ssim\"]\n",
        "            }\n",
        "    # Create the collage\n",
        "    create_multi_model_collage(output_path = output_dir)\n",
        "\n",
        "    print(\"All collages created successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "qxT-V0UVnXDF",
        "outputId": "3c12e558-f84d-4137-e43a-47ac6194bdab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Downloading and preparing DIV2K dataset...\n",
            "DIV2K zip already exists.\n",
            "Using TiledDIV2KDataset\n",
            "Preprocessing 800 HR images for tiling using 8 threads...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Tiling DIV2K: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [00:00<00:00, 7687.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total crop pairs generated: 5052\n",
            "Dataset ready and reproducible.\n",
            "Training model: VDSR_SA with loss: NewCombinedLoss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10:   0%|          | 0/1011 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 194.12 MiB is free. Process 17290 has 14.55 GiB memory in use. Of the allocated memory 14.41 GiB is allocated by PyTorch, and 21.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-20-31611261.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All stages complete.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;31m#=====================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-20-31611261.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# === Step 2: Training ===\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         trained_model, history = train_pipeline(\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-18-2781023710.py\u001b[0m in \u001b[0;36mtrain_pipeline\u001b[0;34m(config, train_loader, val_loader, device)\u001b[0m\n\u001b[1;32m    166\u001b[0m         )\n\u001b[1;32m    167\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         trained_model, history = train_and_validate(\n\u001b[0m\u001b[1;32m    169\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-10-144330551.py\u001b[0m in \u001b[0;36mtrain_and_validate\u001b[0;34m(model, train_loader, val_loader, optimizer, loss_fn, save_dir, checkpoint_dir, model_name, num_epochs, val_fid_interval, device, verbose, early_stopping_patience, use_wandb)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mlr_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhr_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhr_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mlr_up\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhr_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bicubic'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign_corners\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_up\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhr_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4-3021866250.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresblocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 194.12 MiB is free. Process 17290 has 14.55 GiB memory in use. Of the allocated memory 14.41 GiB is allocated by PyTorch, and 21.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "#====================== Main =========================\n",
        "\n",
        "sys.path.append(os.getcwd())\n",
        "def load_config(path=\"config/config.yaml\"):\n",
        "    with open(path, \"r\") as f:\n",
        "        return yaml.safe_load(f)\n",
        "\n",
        "def main():\n",
        "    # === Load Config ===\n",
        "    config = load_config()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # === Optional wandb init ===\n",
        "    if config.get(\"use_wandb\", False):\n",
        "        wandb.init(project=\"super-resolution\", config=config, name=config[\"model\"])\n",
        "\n",
        "    # === Step 1: Preprocessing ===\n",
        "    train_loader, val_loader, test_loader, forced_indices = preprocess_pipeline(config)\n",
        "\n",
        "    # === Step 2: Training ===\n",
        "    if config[\"train\"] == True:\n",
        "        trained_model, history = train_pipeline(\n",
        "            config=config,\n",
        "            train_loader=train_loader,\n",
        "            val_loader=val_loader,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "    # === Step 3: Testing ===\n",
        "    if config[\"test\"] == True:\n",
        "        metrics = test_pipeline(\n",
        "            config=config,\n",
        "            test_loader=test_loader,\n",
        "            forced_indices=forced_indices,\n",
        "            device=device,\n",
        "            history=history\n",
        "        )\n",
        "\n",
        "    print(\"All stages complete.\")\n",
        "\n",
        "main()\n",
        "#====================================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6PMEUB0MuboB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}